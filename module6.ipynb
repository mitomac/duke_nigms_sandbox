{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816ebdc-4114-45e4-baa2-ba48dc6e2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# At the start of your notebook\n",
    "import time\n",
    "notebook_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2977a",
   "metadata": {},
   "source": [
    "# Salmon Module Refactor\n",
    "\n",
    "This notebook demonstrates a cleaner, more modular approach to:\n",
    "1. Installing required bioinformatics tools\n",
    "2. Fetching SRA reads\n",
    "3. Running quality control with `fastp` + `MultiQC`\n",
    "4. Building a Salmon index\n",
    "5. Running Salmon quantification\n",
    "\n",
    "## 1. Install Tools (Optional)\n",
    "If you haven't installed the tools yet, uncomment and run the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d01cf-6d55-4b43-b4f0-d4f59e061fb4",
   "metadata": {},
   "source": [
    "## RNA-seq Pipeline Package Installation\n",
    "\n",
    "This code cell installs essential bioinformatics tools for RNA-seq analysis through conda.\n",
    "\n",
    "### Package Details and Purposes\n",
    "\n",
    "#### Data Access and Download\n",
    "* [**entrez-direct**](https://www.ncbi.nlm.nih.gov/books/NBK179288/) - Command-line tools for querying and downloading from NCBI databases. Essential for programmatically accessing sequence data from repositories like SRA and GenBank. It allows automated batch downloads and queries of NCBI's vast biological databases without manual web interface interaction.\n",
    "* [**sra-tools**](https://github.com/ncbi/sra-tools) - Official NCBI toolkit for working with Sequence Read Archive data. Required for downloading raw sequencing files. It handles the conversion of SRA's specialized format into standard FASTQ files that downstream tools can process.\n",
    "* [**parallel-fastq-dump**](https://github.com/rvalieris/parallel-fastq-dump) - Parallelized version of fastq-dump that significantly speeds up SRA data downloads. While the standard fastq-dump processes files sequentially, this tool utilizes multiple CPU cores to accelerate the download and conversion process.\n",
    "\n",
    "#### Quality Control and Preprocessing\n",
    "* [**fastp**](https://github.com/OpenGene/fastp) - Modern all-in-one FASTQ preprocessor that handles quality control, adapter trimming, and filtering with exceptional speed. Unlike older tools that perform single tasks, fastp integrates multiple preprocessing steps including base quality control, adapter removal, quality filtering, and per-base quality pruning in a single efficient pass through the data.\n",
    "* [**FastQC**](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) - Industry standard tool for generating detailed quality control reports of sequencing data. It provides comprehensive visualizations and statistics about sequence quality scores, GC content, adapter contamination, overrepresented sequences, and other potential quality issues that might affect downstream analysis.\n",
    "* [**MultiQC**](https://multiqc.info/) - Aggregates bioinformatics analysis results across multiple samples into a single comprehensive report. This is particularly valuable for large-scale projects, as it combines reports from various tools (FastQC, Salmon, etc.) across many samples into an interactive HTML report, making it easier to spot batch effects or systematic issues.\n",
    "\n",
    "#### Quantification\n",
    "* [**Salmon**](https://combine-lab.github.io/salmon/) - A cutting-edge tool for transcript quantification that uses lightweight mapping (quasi-mapping) instead of traditional alignment. Unlike traditional aligners (like STAR or HISAT2) that generate BAM files with exact read positions, Salmon uses probabilistic models and lightweight mapping to estimate transcript abundances directly. This approach is substantially faster and more memory-efficient while maintaining high accuracy. It's particularly good at handling technical issues like multi-mapping reads and GC bias.\n",
    "\n",
    "#### Utility\n",
    "* [**pigz**](https://zlib.net/pigz/) - Parallel implementation of gzip for efficient compression/decompression of large sequencing files. While standard gzip uses a single CPU core, pigz parallizes the compression/decompression process across multiple cores, significantly speeding up file operations on modern multi-core systems.\n",
    "\n",
    "The installation uses specific conda channels:\n",
    "* `conda-forge`: General scientific computing packages, known for its comprehensive collection of well-maintained scientific software\n",
    "* `bioconda`: Specialized bioinformatics software channel, maintained by the bioinformatics community with standardized builds and dependencies\n",
    "\n",
    "The `-y` flag automates the installation by accepting all prompts, allowing for unattended installation of all packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -c bioconda \\\n",
    "  entrez-direct fastp fastqc multiqc salmon parallel-fastq-dump sra-tools pigz -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5de13f",
   "metadata": {},
   "source": [
    "## Helper Functions for RNA-seq Data Processing\n",
    "\n",
    "This section contains essential Python functions that help automate and streamline the RNA-seq analysis pipeline. While the implementation details are complex, each function serves a specific purpose in the workflow:\n",
    "\n",
    "### System Setup Functions\n",
    "* `get_cpu_count()` - Intelligently determines how many CPU cores to use for processing, leaving one core free for system operations to prevent overloading.\n",
    "* `setup_directories()` - Creates and organizes the necessary folder structure for storing downloaded and processed sequencing data.\n",
    "\n",
    "### Data Download and Verification\n",
    "* `verify_paired_end()` - Checks whether sequencing data is paired-end (two reads per fragment) or single-end by querying NCBI's metadata.\n",
    "* `check_sra_exists()` - Verifies if sequencing data has already been downloaded in SRA format to prevent redundant downloads.\n",
    "* `verify_fastq_exists()` - Checks if sequencing data has already been converted to FASTQ format to avoid duplicate processing.\n",
    "* `get_srr_dict()` - Retrieves SRA run accessions (SRR numbers) for each sample without downloading data. Creates a mapping between sample groups and their sequencing data.\n",
    "* `get_and_fetch_srr()` - Comprehensive function that:\n",
    " - Downloads raw sequencing data from NCBI\n",
    " - Converts it from SRA to FASTQ format \n",
    " - Compresses files to save space\n",
    " - Handles both single and paired-end data automatically\n",
    "\n",
    "### Data Processing and Quality Control\n",
    "* `run_fastp_with_concat()` - Processes raw sequencing data by:\n",
    " - Combining multiple sequencing runs for the same sample\n",
    " - Trimming low-quality bases\n",
    " - Removing adapter sequences\n",
    " - Generating quality reports\n",
    " - Creating cleaned, high-quality reads for downstream analysis\n",
    "\n",
    "### Quantification\n",
    "* `run_salmon_quant()` - Performs transcript quantification using Salmon by:\n",
    " - Processing cleaned reads from multiple runs per sample\n",
    " - Mapping reads to a reference transcriptome\n",
    " - Generating gene/transcript expression counts\n",
    " - Creating output compatible with downstream differential expression analysis\n",
    "\n",
    "Each function includes error checking and progress reporting to ensure reliable data processing. The functions work together to create a seamless pipeline from raw data download through expression quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24468889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "\n",
    "########################\n",
    "# Helper Functions\n",
    "########################\n",
    "\n",
    "def get_cpu_count():\n",
    "    \"\"\"Get the number of available CPU cores (minus 1).\"\"\"\n",
    "    try:\n",
    "        return max(1, multiprocessing.cpu_count() - 1)\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def setup_directories(base_dir=\".\", sra_dir=\"sra\", fastq_dir=\"fastq\"):\n",
    "    \"\"\"Create and return paths for SRA and fastq storage.\"\"\"\n",
    "    base_path = Path(base_dir).resolve()\n",
    "    sra_path = base_path / sra_dir\n",
    "    fastq_path = base_path / fastq_dir\n",
    "    temp_path = fastq_path / \"temp\"\n",
    "    for path in [sra_path, fastq_path, temp_path]:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    return base_path, sra_path, fastq_path, temp_path\n",
    "\n",
    "def verify_paired_end(srr):\n",
    "    \"\"\"Check if SRR is paired-end by querying SRA metadata.\"\"\"\n",
    "    # SRA: runinfo => layout\n",
    "    cmd1 = f'esearch -db sra -query \"{srr}\" | efetch -format runinfo | cut -d \",\" -f 16'\n",
    "    layout = subprocess.run(cmd1, shell=True, capture_output=True, text=True).stdout.strip()\n",
    "\n",
    "    # Alternatively: docsum => read_spec\n",
    "    cmd2 = (\n",
    "        f'esearch -db sra -query \"{srr}\" | efetch -format docsum ' \n",
    "        f'| xtract -pattern DocumentSummary -element spots.read_spec'\n",
    "    )\n",
    "    read_spec = subprocess.run(cmd2, shell=True, capture_output=True, text=True).stdout.strip()\n",
    "\n",
    "    is_paired = (layout.upper() == \"PAIRED\") or (\"2 reads per spot\" in read_spec)\n",
    "    return is_paired\n",
    "\n",
    "def check_sra_exists(srr, sra_dir):\n",
    "    \"\"\"Check if .sra file already exists locally.\"\"\"\n",
    "    return (Path(sra_dir) / f\"{srr}.sra\").exists()\n",
    "\n",
    "def verify_fastq_exists(srr, output_dir, is_paired):\n",
    "    \"\"\"Check if corresponding FASTQ already exists.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    if is_paired:\n",
    "        expected_files = [f\"{srr}_1.fastq.gz\", f\"{srr}_2.fastq.gz\"]\n",
    "    else:\n",
    "        expected_files = [f\"{srr}.fastq.gz\", f\"{srr}_1.fastq.gz\"]\n",
    "    return any((output_dir / f).exists() for f in expected_files)\n",
    "\n",
    "def get_srr_dict(gsm_list):\n",
    "    \"\"\"\n",
    "    Get SRR numbers for each GSM, without downloading.\n",
    "    Returns a dict: { group_name: [SRR1, SRR2, ...], ... }.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for gsm, group in gsm_list:\n",
    "        cmd1 = (\n",
    "            f'esearch -db gds -query \"{gsm}\" | efetch -format docsum | '\n",
    "            f'xtract -pattern ExtRelation -element TargetObject | grep ^SRX'\n",
    "        )\n",
    "        srx = subprocess.run(cmd1, shell=True, capture_output=True, text=True).stdout.strip()\n",
    "\n",
    "        cmd2 = (\n",
    "            f'esearch -db sra -query \"{srx}\" | efetch -format runinfo | '\n",
    "            f'cut -d \",\" -f 1 | grep ^SRR'\n",
    "        )\n",
    "        srr_list = subprocess.run(cmd2, shell=True, capture_output=True, text=True).stdout.split()\n",
    "        results[group] = srr_list\n",
    "    return results\n",
    "\n",
    "def get_and_fetch_srr(gsm_list, base_dir=\".\", sra_dir=\"sra\", output_dir=\"fastq\"):\n",
    "    \"\"\"\n",
    "    Fetch SRRs for each GSM => prefetch => convert to FASTQ => compress.\n",
    "    Returns a dict: { group_name: [SRR1, SRR2, ...], ...}.\n",
    "    \"\"\"\n",
    "    base_path, sra_path, fastq_path, temp_path = setup_directories(base_dir, sra_dir, output_dir)\n",
    "    results = {}\n",
    "    cpu_count = get_cpu_count()\n",
    "\n",
    "    for gsm, group in gsm_list:\n",
    "        # Get SRX\n",
    "        cmd_srx = (\n",
    "            f'esearch -db gds -query \"{gsm}\" | efetch -format docsum | '\n",
    "            f'xtract -pattern ExtRelation -element TargetObject | grep ^SRX'\n",
    "        )\n",
    "        srx = subprocess.run(cmd_srx, shell=True, capture_output=True, text=True).stdout.strip()\n",
    "\n",
    "        # Get SRRs\n",
    "        cmd_srr = (\n",
    "            f'esearch -db sra -query \"{srx}\" | efetch -format runinfo | '\n",
    "            f'cut -d \",\" -f 1 | grep ^SRR'\n",
    "        )\n",
    "        srr_list = subprocess.run(cmd_srr, shell=True, capture_output=True, text=True).stdout.split()\n",
    "        results[group] = srr_list\n",
    "\n",
    "        for srr in srr_list:\n",
    "            is_paired = verify_paired_end(srr)\n",
    "            if verify_fastq_exists(srr, fastq_path, is_paired):\n",
    "                continue\n",
    "            # Prefetch if missing\n",
    "            if not check_sra_exists(srr, sra_path):\n",
    "                subprocess.run(f'prefetch --progress {srr} --output-directory {sra_path}', shell=True)\n",
    "            # Convert to fastq\n",
    "            cmd_fastq = (\n",
    "                f'fasterq-dump --split-files {srr} -O {fastq_path} '\n",
    "                f'-p --threads {cpu_count} --temp {temp_path}'\n",
    "            )\n",
    "            subprocess.run(cmd_fastq, shell=True)\n",
    "\n",
    "            # Compress\n",
    "            potential_files = list(fastq_path.glob(f\"{srr}*.fastq\"))\n",
    "            for fq in potential_files:\n",
    "                subprocess.run(f'pigz -p {cpu_count} {fq}', shell=True)\n",
    "\n",
    "    # Cleanup\n",
    "    shutil.rmtree(temp_path)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def run_fastp_with_concat(\n",
    "    srr_dict,\n",
    "    fastq_dir=\"fastq\",\n",
    "    trimmed_dir=\"trimmed_reads\",\n",
    "    report_dir=\"fastp_reports\",\n",
    "    phred_cutoff=15,\n",
    "    min_length=36,\n",
    "    cpu_count=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Concatenate all FASTQs for each experiment (group) and run fastp trimming.\n",
    "    \n",
    "    Args:\n",
    "        srr_dict (dict): Dictionary mapping { experiment_name: [SRR1, SRR2, ...], ... }\n",
    "        fastq_dir (str): Directory containing raw FASTQ files (*.fastq.gz)\n",
    "        trimmed_dir (str): Output directory for trimmed FASTQs\n",
    "        report_dir (str): Output directory for fastp JSON/HTML reports\n",
    "        phred_cutoff (int): Phred quality cutoff for trimming\n",
    "        min_length (int): Minimum length required after trimming\n",
    "        cpu_count (int): Number of threads to use for fastp\n",
    "    \"\"\"\n",
    "    fastq_dir = Path(fastq_dir)\n",
    "    trimmed_dir = Path(trimmed_dir)\n",
    "    report_dir = Path(report_dir)\n",
    "    \n",
    "    trimmed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    report_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for experiment_name, srr_list in srr_dict.items():\n",
    "        print(f\"\\nProcessing experiment: {experiment_name}\")\n",
    "        \n",
    "        # Collect all FASTQ files for this experiment.\n",
    "        # For single-end data, you might find each SRR as {SRR}_1.fastq.gz or {SRR}.fastq.gz\n",
    "        # Adjust logic as needed for your naming convention.\n",
    "        fastq_paths = []\n",
    "        for srr in srr_list:\n",
    "            # Potential naming patterns (adjust to your needs):\n",
    "            possible_paths = [\n",
    "                fastq_dir / f\"{srr}.fastq.gz\",\n",
    "                fastq_dir / f\"{srr}_1.fastq.gz\"\n",
    "            ]\n",
    "            # Collect whichever actually exists\n",
    "            existing = [p for p in possible_paths if p.exists()]\n",
    "            if not existing:\n",
    "                print(f\"  Warning: No FASTQ file found for {srr}\")\n",
    "            else:\n",
    "                fastq_paths.extend(existing)\n",
    "        \n",
    "        if not fastq_paths:\n",
    "            print(f\"  No FASTQs to process for {experiment_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Concatenate them into a single file: e.g., \"my_experiment_untrimmed.fastq.gz\"\n",
    "        # We'll zcat each input into one combined FASTQ so that fastp only sees one file.\n",
    "        combined_fastq = trimmed_dir / f\"{experiment_name}_untrimmed.fastq.gz\"\n",
    "        \n",
    "        ncores = get_cpu_count()  # Reuse our CPU counting function\n",
    "\n",
    "        if len(fastq_paths) == 1:\n",
    "            # If there's only one file, just copy it (instead of doing zcat).\n",
    "            shutil.copy(fastq_paths[0], combined_fastq)\n",
    "            print(f\"  Copied single FASTQ => {combined_fastq.name}\")\n",
    "        else:\n",
    "            # Multiple FASTQs => merge them using pigz for parallel compression\n",
    "            cmd_concat = (\n",
    "                f\"zcat {' '.join(str(p) for p in fastq_paths)} | \"\n",
    "                f\"pigz -p {ncores} > {combined_fastq}\"\n",
    "            )\n",
    "            print(f\"  Merging {len(fastq_paths)} FASTQs into => {combined_fastq.name}\")\n",
    "            subprocess.run(cmd_concat, shell=True, check=True)\n",
    "        \n",
    "        # Now run fastp on the combined FASTQ\n",
    "        trimmed_fastq = trimmed_dir / f\"{experiment_name}.trimmed.fastq.gz\"\n",
    "        json_report = report_dir / f\"{experiment_name}.json\"\n",
    "        html_report = report_dir / f\"{experiment_name}.html\"\n",
    "        \n",
    "        cmd_fastp = [\n",
    "            \"fastp\",\n",
    "            \"--in1\", str(combined_fastq),\n",
    "            \"--out1\", str(trimmed_fastq),\n",
    "            \"--json\", str(json_report),\n",
    "            \"--html\", str(html_report),\n",
    "            \"--thread\", str(cpu_count),\n",
    "            \"--qualified_quality_phred\", str(phred_cutoff),\n",
    "            \"--length_required\", str(min_length),\n",
    "            \"--cut_right\",\n",
    "            \"--cut_right_window_size\", \"4\",\n",
    "            \"--cut_right_mean_quality\", str(phred_cutoff)\n",
    "        ]\n",
    "        \n",
    "        print(f\"  Running fastp => {experiment_name}.trimmed.fastq.gz\")\n",
    "        subprocess.run(cmd_fastp, check=True)\n",
    "        \n",
    "        # (Optional) remove the untrimmed file if you don't need it anymore\n",
    "        if combined_fastq.exists():\n",
    "            combined_fastq.unlink()\n",
    "    \n",
    "    print(\"\\nDone! Now you can run MultiQC on the fastp_reports/ directory.\")\n",
    "\n",
    "\n",
    "def run_salmon_quant(srr_dict, salmon_index=\"salmon/salmon_index\", output_dir=\"salmon_quant\"):\n",
    "    \"\"\"\n",
    "    Run Salmon quantification on all samples, combining multiple fastqs per group.\n",
    "    Expects trimmed FASTQs in `trimmed_reads/`.\n",
    "    \"\"\"\n",
    "    ncores = get_cpu_count()\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for exp_name, srr_list in srr_dict.items():\n",
    "        fastq_files = []\n",
    "        for srr in srr_list:\n",
    "            # Adjust naming as needed\n",
    "            fastq_path = f\"trimmed_reads/{srr}_1.trimmed.fastq.gz\"\n",
    "            if os.path.exists(fastq_path):\n",
    "                fastq_files.append(fastq_path)\n",
    "            else:\n",
    "                print(f\"Warning: {fastq_path} not found.\")\n",
    "\n",
    "        if fastq_files:\n",
    "            cmd = [\n",
    "                \"salmon\", \"quant\",\n",
    "                \"-i\", salmon_index,\n",
    "                \"-l\", \"SR\",  # single-end\n",
    "                \"-p\", str(ncores),\n",
    "                \"--validateMappings\",\n",
    "                \"-o\", f\"{output_dir}/{exp_name}\"\n",
    "            ]\n",
    "            # Add each FASTQ with -r\n",
    "            for fastq in fastq_files:\n",
    "                cmd.extend([\"-r\", fastq])\n",
    "\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error processing {exp_name}\")\n",
    "                print(result.stderr)\n",
    "            else:\n",
    "                print(f\"Salmon quant finished for {exp_name}!\")\n",
    "        else:\n",
    "            print(f\"No FASTQ files found for {exp_name}\")\n",
    "    print(\"\\nAll Salmon quantifications are done!\")\n",
    "\n",
    "def run_salmon_quant_new(srr_dict, salmon_index=\"salmon/salmon_index\", output_dir=\"salmon_quant\"):\n",
    "    \"\"\"\n",
    "    Run Salmon quantification on all samples.\n",
    "    Expects trimmed FASTQs in `trimmed_reads/` named by experiment.\n",
    "    \"\"\"\n",
    "    ncores = get_cpu_count()\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for exp_name in srr_dict.keys():\n",
    "        # Look for the concatenated, trimmed fastq created by run_fastp_with_concat\n",
    "        fastq_path = f\"trimmed_reads/{exp_name}.trimmed.fastq.gz\"\n",
    "        \n",
    "        if os.path.exists(fastq_path):\n",
    "            cmd = [\n",
    "                \"salmon\", \"quant\",\n",
    "                \"-i\", salmon_index,\n",
    "                \"-l\", \"SR\",  # single-end\n",
    "                \"-p\", str(ncores),\n",
    "                \"--validateMappings\",\n",
    "                \"-o\", f\"{output_dir}/{exp_name}\",\n",
    "                \"-r\", fastq_path  # Single concatenated file per experiment\n",
    "            ]\n",
    "\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if result.returncode != 0:\n",
    "                print(f\"Error processing {exp_name}\")\n",
    "                print(result.stderr)\n",
    "            else:\n",
    "                print(f\"Salmon quant finished for {exp_name}!\")\n",
    "        else:\n",
    "            print(f\"Warning: No trimmed FASTQ found for {exp_name} at {fastq_path}\")\n",
    "    \n",
    "    print(\"\\nAll Salmon quantifications are done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8aa3f4",
   "metadata": {},
   "source": [
    "## 3. Example: Fetch SRA Reads\n",
    "Below is an example of how you might call the functions to fetch reads by GSM accession. Adjust `gsm_data` to your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96b8d8-e8cb-4c0c-a2d2-0e4dd601da57",
   "metadata": {},
   "source": [
    "## Experimental Design: SARS-CoV-2 Infection in Cell Lines\n",
    "\n",
    "This data comes from a pivotal study ([Blanco-Melo et al., Cell 2020](https://doi.org/10.1016/j.cell.2020.04.026)) examining SARS-CoV-2 infection responses in cell culture models. This seminal paper, \"Imbalanced Host Response to SARS-CoV-2 Drives Development of COVID-19\", was one of the first to characterize the unique host response to SARS-CoV-2 and has been cited over 2,500 times. The experiment uses A549 cells, a human lung cancer cell line commonly used in respiratory virus research.\n",
    "\n",
    "### Sample Groups\n",
    "The experiment includes two main series of comparisons:\n",
    "\n",
    "#### Series 5: Standard A549 Cells\n",
    "* **Treatment Group**: SARS-CoV-2 infected (n=3)\n",
    " - GSM4462339, GSM4462340, GSM4462341\n",
    "* **Control Group**: Mock infected (n=3)\n",
    " - GSM4462336, GSM4462337, GSM4462338\n",
    "\n",
    "#### Series 16: Modified A549-ACE2 Cells\n",
    "These cells were engineered to express the ACE2 receptor, which SARS-CoV-2 uses to enter cells.\n",
    "* **Treatment Group**: SARS-CoV-2 infected (n=3)\n",
    " - GSM4486160, GSM4486161, GSM4486162\n",
    "* **Control Group**: Mock infected (n=3)\n",
    " - GSM4486157, GSM4486158, GSM4486159\n",
    "\n",
    "### Experimental Significance\n",
    "* A549 cells normally express very low levels of ACE2, the receptor SARS-CoV-2 uses to enter cells\n",
    "* By comparing standard A549 cells with ACE2-expressing A549 cells, researchers can:\n",
    " - Understand how the virus affects cells when it can and cannot efficiently enter\n",
    " - Identify cellular responses specific to viral entry versus exposure\n",
    " - Study how ACE2 expression levels impact infection outcomes\n",
    "\n",
    "### Data Structure and Storage\n",
    "* Each sample has a unique GEO accession (GSM) number for metadata tracking\n",
    "* Each GSM is associated with one or more SRA run accessions (SRR numbers)\n",
    "* SRR files are stored in a specialized `.sra` format:\n",
    " - Binary format that efficiently stores sequencing reads\n",
    " - Contains both sequence data and quality scores\n",
    " - Requires special tools (SRA Toolkit) to convert to standard FASTQ format\n",
    " - More space-efficient than raw FASTQ files\n",
    " - Can represent both single-end and paired-end sequencing data\n",
    " - Acts as an archive format for sequence repositories\n",
    "\n",
    "### Data Processing Flow\n",
    "1. GSM accessions identify the biological samples\n",
    "2. SRR numbers link to the actual sequencing data\n",
    "3. `.sra` files are downloaded using `prefetch`\n",
    "4. `.sra` files are converted to FASTQ using `fasterq-dump`\n",
    "5. FASTQ files are then processed for quality control and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719f2c4-28ce-4550-901d-1b4d71d4aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_data = [\n",
    "    (\"GSM4462339\", \"Series5_A549_SARS-CoV-2_1\"),\n",
    "    (\"GSM4462340\", \"Series5_A549_SARS-CoV-2_2\"),\n",
    "    (\"GSM4462341\", \"Series5_A549_SARS-CoV-2_3\"),\n",
    "    (\"GSM4462336\", \"Series5_A549_Mock_1\"),\n",
    "    (\"GSM4462337\", \"Series5_A549_Mock_2\"),\n",
    "    (\"GSM4462338\", \"Series5_A549_Mock_3\"),\n",
    "    (\"GSM4486162\", \"Series16_A549-ACE2_SARS-CoV-2_3\"),\n",
    "    (\"GSM4486161\", \"Series16_A549-ACE2_SARS-CoV-2_2\"),\n",
    "    (\"GSM4486160\", \"Series16_A549-ACE2_SARS-CoV-2_1\"),\n",
    "    (\"GSM4486159\", \"Series16_A549-ACE2_Mock_3\"),\n",
    "    (\"GSM4486158\", \"Series16_A549-ACE2_Mock_2\"),\n",
    "    (\"GSM4486157\", \"Series16_A549-ACE2_Mock_1\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Get the SRR numbers and fetch the files\n",
    "#srr_dict = get_and_fetch_srr(gsm_data, output_dir=\"fastq\")\n",
    "srr_dict = get_and_fetch_srr(gsm_data, output_dir=\"fastq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070e97f-57dd-4c00-bdd8-f43df76dee5f",
   "metadata": {},
   "source": [
    "## Understanding FASTQ Files in RNA-seq\n",
    "\n",
    "FASTQ files are the standard format for storing raw sequencing data in RNA-seq experiments. Each sequencing read in a FASTQ file consists of four lines:\n",
    "\n",
    "### FASTQ File Structure\n",
    "1. **Sequence Identifier** (starts with '@')\n",
    "  - Contains instrument ID, run information, and coordinates\n",
    "  - Example: `@SRR10971381.1 HWUSI-EAS100R:6:73:941:1973/1`\n",
    "\n",
    "2. **Raw Sequence**\n",
    "  - The actual DNA/RNA sequence using A, C, G, T (or U)\n",
    "  - Example: `ATCGTAGCTACGATCGACTGACTGACTACG`\n",
    "\n",
    "3. **Description Line** (starts with '+')\n",
    "  - Often just a '+' or repeats the identifier\n",
    "  - Separates sequence from quality scores\n",
    "\n",
    "4. **Quality Scores** (Phred scores)\n",
    "  - ASCII-encoded quality values for each base\n",
    "  - Higher scores indicate more reliable base calls\n",
    "  - Example: `@@CCFFFFFHHHHHJJJJJJJJJJJJJJJJ`\n",
    "\n",
    "### Example FASTQ Entry\n",
    "\n",
    "```\n",
    "@SRR10971381.1 HWUSI-EAS100R:6:73:941:1973/1\n",
    "ATCGTAGCTACGATCGACTGACTGACTACG\n",
    "+\n",
    "@@CCFFFFFHHHHHJJJJJJJJJJJJJJJJ\n",
    "```\n",
    "\n",
    "## FASTQ Quality Scores: ASCII Encoding\n",
    "\n",
    "Quality scores in FASTQ files are encoded using ASCII characters to save space and improve readability. Each base's quality score is represented by a single ASCII character.\n",
    "\n",
    "### How It Works\n",
    "* Quality scores (Phred scores) range from 0 to 40+\n",
    "* These numbers are converted to ASCII characters by adding an offset\n",
    "* Two common encoding schemes:\n",
    "  - Phred+33: Offset of 33 (modern standard)\n",
    "  - Phred+64: Offset of 64 (older Illumina format)\n",
    "\n",
    "### Phred+33 Quality Score Examples\n",
    "\n",
    "| Quality Score (Q) | ASCII Value | ASCII Character | Error Probability |\n",
    "|------------------|-------------|-----------------|------------------|\n",
    "| 40               | 73          | I              | 0.0001%         |\n",
    "| 30               | 63          | ?              | 0.1%            |\n",
    "| 20               | 53          | 5              | 1%              |\n",
    "| 10               | 43          | +              | 10%             |\n",
    "\n",
    "### Common Quality Ranges\n",
    "\n",
    "| ASCII Character | Quality Range        | Meaning                          |\n",
    "|----------------|---------------------|----------------------------------|\n",
    "| !              | Worst (Q=0)         | Definite read error              |\n",
    "| # to '         | Poor (Q<20)         | Low quality, consider trimming   |\n",
    "| 5 to ?         | Good (Q20-30)       | Good quality for most applications|\n",
    "| @ to I         | Excellent (Q30-40)  | Very high quality reads          |\n",
    "\n",
    "So when you see a quality line like:\n",
    "\n",
    "```\n",
    "@@CCFFFFFHHHHHJJJJJJJJJJJJJJJJ\n",
    "\n",
    "```\n",
    "\n",
    "Each character represents the quality of the corresponding base in the sequence, with higher ASCII characters indicating better quality.\n",
    "\n",
    "### File Processing\n",
    "* FASTQ files are typically compressed (`.fastq.gz`) due to their large size\n",
    "* A typical RNA-seq experiment can generate millions of reads\n",
    "* Quality control tools like FastQC and fastp analyze these files to:\n",
    "  - Identify poor quality regions\n",
    "  - Detect adapter contamination\n",
    "  - Find potential sequencing problems\n",
    "  - Guide preprocessing decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a7b8b",
   "metadata": {},
   "source": [
    "## Building a Transcriptome Index with Salmon\n",
    "\n",
    "Salmon uses a novel indexing and quantification strategy that makes it both fast and accurate for RNA-seq analysis. Unlike traditional aligners that create BAM files with exact read positions, Salmon builds an index optimized for quasi-mapping and transcript quantification.\n",
    "\n",
    "### The Reference Transcriptome\n",
    "* We need a transcriptome FASTA file (not a genome)\n",
    " - Usually downloaded from Ensembl, GENCODE, or RefSeq\n",
    " - Contains sequences for all known transcripts\n",
    " - Each sequence represents a mature, spliced transcript\n",
    " - Includes both protein-coding and non-coding RNAs\n",
    "\n",
    "### Salmon Index Features\n",
    "* **Quasi-mapping** approach:\n",
    " - Creates a minimal perfect hash of k-mers from transcripts\n",
    " - Enables ultra-fast mapping without full alignment\n",
    " - Reduces computational overhead significantly\n",
    "* **Dual-phase indexing**:\n",
    " - First phase builds a sequence dictionary\n",
    " - Second phase creates efficient search structures\n",
    "* **Built-in sequence deduplication**:\n",
    " - Identifies and handles repeated sequences\n",
    " - Important for gene families and paralogs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e738250-2628-48e0-86a1-4e0cd904d362",
   "metadata": {},
   "source": [
    "## Downloading Reference Data from GENCODE\n",
    "\n",
    "This code block downloads essential reference files for human genome analysis from GENCODE, a high-quality gene annotation database.\n",
    "\n",
    "### Command Breakdown\n",
    "\n",
    "1. **Create Directory**\n",
    "* `mkdir -p genome`\n",
    "* Creates a new directory called `genome`\n",
    "* `-p` flag creates parent directories if needed and prevents errors if directory exists\n",
    "\n",
    "2. **Download Reference Genome**\n",
    "* Downloads the human reference genome sequence (GRCh38)\n",
    "* `primary_assembly` means it includes:\n",
    "  - Standard chromosomes (1-22, X, Y, M)\n",
    "  - No alternate haplotypes or patches\n",
    "* `.fa.gz` indicates a compressed FASTA file\n",
    "\n",
    "3. **Download Transcriptome**\n",
    "* Contains sequences of mature transcripts\n",
    "* Includes all known transcript variants\n",
    "* Used for RNA-seq quantification with Salmon\n",
    "* Version 47 is one of the latest GENCODE releases\n",
    "\n",
    "4. **Download Gene Annotations**\n",
    "* GTF file containing gene structure information\n",
    "* `basic` set includes:\n",
    "  - Verified, well-supported gene models\n",
    "  - Simplified version of full annotation\n",
    "  - Recommended for most analyses\n",
    "* Contains information about:\n",
    "  - Gene locations\n",
    "  - Exon boundaries\n",
    "  - Transcript structures\n",
    "  - Gene names and IDs\n",
    "\n",
    "### File Usage\n",
    "* Genome (.fa.gz) → Reference for variant calling\n",
    "* Transcriptome (.fa.gz) → Input for Salmon index\n",
    "* Annotations (.gtf.gz) → Gene/transcript information for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9e813-7eb0-4c8a-b028-807694bc04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p genome\n",
    "!wget -P genome ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/GRCh38.primary_assembly.genome.fa.gz\n",
    "!wget -P genome ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/gencode.v47.transcripts.fa.gz\n",
    "!wget -P genome ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/gencode.v47.basic.annotation.gtf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efd9d4-b5e7-489d-9302-0c824a7fdbac",
   "metadata": {},
   "source": [
    "## Building a Salmon Index with Decoy Sequences\n",
    "\n",
    "The following commands build a Salmon index that includes decoy sequences to improve mapping accuracy.\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. **Create Salmon Directory**\n",
    "* Makes a directory to store index and related files\n",
    "\n",
    "2. **Extract Decoy Sequences**\n",
    "* Extracts chromosome names from the genome FASTA\n",
    "* These will serve as \"decoy\" sequences to prevent false transcript mappings\n",
    "* Command breakdown:\n",
    " - `gunzip -c` decompresses and outputs to stdout\n",
    " - `grep \"^>\"` finds all FASTA headers\n",
    " - `cut -d \" \" -f 1` extracts just the sequence names\n",
    " - Result saved in `decoys.txt`\n",
    "\n",
    "3. **Clean Decoy List**\n",
    "* Removes the '>' characters from FASTA headers\n",
    "* Creates a clean list of chromosome names\n",
    "* Keeps backup with `.bak` extension\n",
    "\n",
    "4. **Create Gentrome**\n",
    "* Concatenates transcriptome and genome files\n",
    "* Called \"gentrome\" (genome + transcriptome)\n",
    "* Helps Salmon distinguish between:\n",
    " - True transcript mappings\n",
    " - Genomic DNA contamination\n",
    " - Incomplete splicing\n",
    "\n",
    "5. **Build Salmon Index**\n",
    "* `-t gentrome.fa.gz`: Combined transcriptome and genome\n",
    "* `-d decoys.txt`: List of genomic decoy sequences\n",
    "* `-p 60`: Use 60 threads (adjust based on your system)\n",
    "* `-i salmon_index`: Output directory\n",
    "* `--gencode`: Use GENCODE-specific parsing\n",
    "\n",
    "### Why Use Decoys?\n",
    "* Improves accuracy by reducing false mappings\n",
    "* Helps handle:\n",
    " - Genomic DNA contamination\n",
    " - Unspliced RNA\n",
    " - Incomplete transcript annotations\n",
    "* Particularly important for:\n",
    " - Novel transcript discovery\n",
    " - Alternative splicing analysis\n",
    " - Low-quality RNA samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d791dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example commands:\n",
    "!mkdir -p salmon\n",
    "!grep \"^>\" <(gunzip -c genome/GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 > salmon/decoys.txt\n",
    "!sed -i.bak -e 's/>//g' salmon/decoys.txt\n",
    "!cat genome/gencode.v47.transcripts.fa.gz genome/GRCh38.primary_assembly.genome.fa.gz > salmon/gentrome.fa.gz\n",
    "!cd salmon && salmon index -t gentrome.fa.gz -d decoys.txt -p 60 -i salmon_index --gencode\n",
    "\n",
    "print(\"Salmon index building commands go here. Uncomment and adjust as needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68020ca4",
   "metadata": {},
   "source": [
    "## Quality Control with fastp and MultiQC\n",
    "\n",
    "Before quantification, raw sequencing reads need quality control and preprocessing. We use two powerful tools: fastp for read processing and MultiQC for quality reporting.\n",
    "\n",
    "### fastp: Modern FASTQ Preprocessing\n",
    "`fastp` performs multiple essential steps in a single, efficient pass through the data:\n",
    "\n",
    "1. **Quality Control Metrics**\n",
    "  * Base quality distribution\n",
    "  * Sequence content analysis\n",
    "  * GC content distribution\n",
    "  * Sequence length distribution\n",
    "  * Duplication rate analysis\n",
    "\n",
    "2. **Read Trimming Actions**\n",
    "  * Removes low-quality bases (below Phred score threshold)\n",
    "  * Trims adapter sequences automatically\n",
    "  * Removes N bases\n",
    "  * Filters reads by length and complexity\n",
    "  * Performs sliding window quality trimming\n",
    "\n",
    "3. **Key Parameters We Use**\n",
    "  * `--qualified_quality_phred 15`: Minimum base quality score\n",
    "  * `--length_required 36`: Minimum read length after trimming\n",
    "  * `--cut_right`: Enable sliding window trimming\n",
    "  * `--cut_right_window_size 4`: Window size for quality checking\n",
    "  * `--thread`: Utilize multiple CPU cores\n",
    "\n",
    "### MultiQC: Aggregating Quality Reports\n",
    "\n",
    "MultiQC collects reports from multiple samples and tools to create a comprehensive quality overview:\n",
    "\n",
    "1. **Data Collection**\n",
    "  * Scans directories recursively\n",
    "  * Finds reports from various bioinformatics tools\n",
    "  * Recognizes fastp JSON reports automatically\n",
    "\n",
    "2. **Generated Reports Include**\n",
    "  * Interactive HTML dashboard\n",
    "  * Sample comparison plots\n",
    "  * Quality metrics across all samples\n",
    "  * Flags potential quality issues\n",
    "\n",
    "3. **Key Visualizations**\n",
    "  * Per-base sequence quality\n",
    "  * Sequence length distributions\n",
    "  * Adapter content\n",
    "  * Duplication rates\n",
    "  * GC content\n",
    "  * Quality score distributions\n",
    "\n",
    "### Quality Assessment Flow\n",
    "1. Raw reads → fastp → Trimmed reads\n",
    "2. fastp generates per-sample reports\n",
    "3. MultiQC combines all reports\n",
    "4. Review MultiQC dashboard:\n",
    "  * Look for systematic quality issues\n",
    "  * Compare samples for batch effects\n",
    "  * Identify potential outliers\n",
    "  * Verify trimming effectiveness\n",
    "\n",
    "The combination of fastp's efficient processing and MultiQC's comprehensive reporting provides a robust quality control pipeline for RNA-seq data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416891f-b620-49db-9244-43ebb56d9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose you already have a dictionary: srr_dict = { \"exp1\": [\"SRR123\", \"SRR124\"], \"exp2\": [\"SRR999\"] }\n",
    "# And you have raw FASTQ files in 'fastq/' named SRR123.fastq.gz, SRR124.fastq.gz, SRR999.fastq.gz, etc.\n",
    "\n",
    "# Then just do:\n",
    "run_fastp_with_concat(\n",
    "    srr_dict,\n",
    "    fastq_dir=\"fastq\",\n",
    "    trimmed_dir=\"trimmed_reads\",\n",
    "    report_dir=\"fastp_reports\",\n",
    "    phred_cutoff=15,\n",
    "    min_length=36,\n",
    "    cpu_count=16\n",
    ")\n",
    "\n",
    "# After that, run MultiQC in a separate cell:\n",
    "# !multiqc fastp_reports --filename multiqc_fastp_report --title \"Fastp QC Report\" -o fastp_reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65822744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code snippet for fastp + MultiQC (uncomment to use)\n",
    "\n",
    "# import subprocess\n",
    "# from pathlib import Path\n",
    "#\n",
    "fastq_dir = \"fastq\"\n",
    "trimmed_dir = \"trimmed_reads\"\n",
    "report_dir = \"fastp_reports\"\n",
    "#\n",
    "Path(trimmed_dir).mkdir(exist_ok=True)\n",
    "Path(report_dir).mkdir(exist_ok=True)\n",
    "cpu_count = get_cpu_count()\n",
    "#\n",
    "for fastq_path in Path(fastq_dir).glob(\"*.fastq.gz\"):\n",
    "     sample = fastq_path.stem.replace('.fastq', '')\n",
    "     out_path = Path(trimmed_dir) / f\"{sample}.trimmed.fastq.gz\"\n",
    "     json_path = Path(report_dir) / f\"{sample}.json\"\n",
    "     html_path = Path(report_dir) / f\"{sample}.html\"\n",
    "     cmd = [\n",
    "         \"fastp\",\n",
    "         \"--in1\", str(fastq_path),\n",
    "         \"--out1\", str(out_path),\n",
    "         \"--json\", str(json_path),\n",
    "         \"--html\", str(html_path),\n",
    "         \"--thread\", str(cpu_count),\n",
    "         \"--qualified_quality_phred\", \"15\",\n",
    "         \"--length_required\", \"36\",\n",
    "         \"--cut_right\",\n",
    "         \"--cut_right_window_size\", \"4\",\n",
    "         \"--cut_right_mean_quality\", \"15\"\n",
    "     ]\n",
    "     subprocess.run(cmd)\n",
    "#\n",
    "# Then run MultiQC\n",
    "!multiqc fastp_reports --filename multiqc_fastp_report --title \"Fastp QC Report\" -o fastp_reports\n",
    "\n",
    "print(\"Example fastp + MultiQC code. Uncomment & adjust to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386f22c-20b8-4c6b-ac31-541f85e1a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!multiqc fastp_reports --filename multiqc_fastp_report --title \"Fastp QC Report\" -o fastp_reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b610be96",
   "metadata": {},
   "source": [
    "## Salmon Quantification and Expression Estimation\n",
    "\n",
    "Salmon performs transcript quantification through a two-phase process that produces accurate abundance estimates without traditional alignment.\n",
    "\n",
    "### Understanding Salmon Quantification\n",
    "\n",
    "1. **Mapping Phase**\n",
    "* Uses quasi-mapping instead of traditional alignment\n",
    "* Maps reads to transcripts using the pre-built index\n",
    "* Much faster than traditional aligners\n",
    "* No BAM files are generated\n",
    "\n",
    "2. **Quantification Phase**\n",
    "* Employs an expectation-maximization (EM) algorithm\n",
    "* Resolves multi-mapping reads probabilistically\n",
    "* Accounts for positional and sequence-specific biases\n",
    "* Estimates transcript abundances in TPM and counts\n",
    "\n",
    "### Key Parameters Used\n",
    "* `-i salmon_index`: Path to pre-built Salmon index\n",
    "* `-l SR`: Library type (SR = single-read)\n",
    "* `-p`: Number of threads for parallel processing\n",
    "* `--validateMappings`: Perform additional mapping validation\n",
    "* `-r`: Input trimmed FASTQ file(s)\n",
    "\n",
    "### Output Files (.sf format)\n",
    "Each quantification produces a `quant.sf` file containing:\n",
    "* `Name`: Transcript identifier\n",
    "* `Length`: Transcript length\n",
    "* `EffectiveLength`: Length adjusted for fragment size distribution\n",
    "* `TPM`: Transcripts Per Million\n",
    "* `NumReads`: Estimated number of reads mapping to transcript\n",
    "\n",
    "### Sample Output Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62a209-1d03-4ca9-aa69-b9bb4c808446",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head salmon_quant/Series5_A549_Mock_1/quant.sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec00d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Suppose we have a dictionary of groups -> SRR IDs from get_and_fetch_srr:\n",
    "# srr_dict = {\n",
    "#   \"Series5_A549_SARS-CoV-2_1\": [\"SRR11517677\"],\n",
    "#   \"Series5_A549_SARS-CoV-2_2\": [\"SRR11517678\"],\n",
    "#   \"Series5_A549_SARS-CoV-2_3\": [\"SRR11517679\"],\n",
    "#   \"Series5_A549_Mock_1\"     : [\"SRR11517674\"],\n",
    "#   \"Series5_A549_Mock_2\"     : [\"SRR11517675\"],\n",
    "#   \"Series5_A549_Mock_3\"     : [\"SRR11517676\"]\n",
    "# }\n",
    "#\n",
    "run_salmon_quant_new(srr_dict, salmon_index=\"salmon/salmon_index\", output_dir=\"salmon_quant\")\n",
    "\n",
    "print(\"Example Salmon quant usage. Uncomment & adjust the dictionary + paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f82725-706f-4135-87a6-9b4c82fec5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end of your notebook\n",
    "notebook_end_time = time.time()\n",
    "print(f\"Total notebook execution time: {(notebook_end_time - notebook_start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cb1b3-3318-4fc6-b4a8-1a690639a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dictionary to dataframe\n",
    "df = pd.DataFrame({\n",
    "    'sample_id': list(srr_dict.keys()),\n",
    "    'srr_numbers': list(srr_dict.values())\n",
    "})\n",
    "\n",
    "# Extract information from sample_id\n",
    "df['condition'] = df['sample_id'].str.extract('_(Mock|SARS-CoV-2)_')\n",
    "df['cell_type'] = df['sample_id'].str.extract('_(A549-ACE2|A549)_')\n",
    "\n",
    "# Sort to match your desired order\n",
    "df = df.sort_values(['cell_type', 'condition', 'sample_id'])\n",
    "\n",
    "# Select and order columns\n",
    "df_final = df[['sample_id', 'condition', 'cell_type']]\n",
    "\n",
    "# Save to CSV\n",
    "df_final.to_csv('sample_info.csv', index=False)\n",
    "\n",
    "# Print preview\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a85326-99d3-4d98-9151-b028b85a1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "srr_dict"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
